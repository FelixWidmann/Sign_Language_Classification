{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder, LabelBinarizer\n",
    "import string\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.metrics import f1_score\n",
    "import wandb\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"Torch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_train = pd.read_csv(\"./data/sign_mnist_train.csv\")\n",
    "sign_test = pd.read_csv(\"./data/sign_mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize Examples per Label in Training- and Test-Dataset\n",
    "train_counts = sign_train['label'].value_counts().sort_index()\n",
    "test_counts = sign_test['label'].value_counts().sort_index()\n",
    "\n",
    "labels = list(set(sign_train[\"label\"]))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.bar(labels, train_counts, alpha=0.5, color='b', label='Train')\n",
    "plt.bar(labels, test_counts, alpha=0.5, color='r', label='Test')\n",
    "\n",
    "plt.title('Verteilung der Trainings- und Testbeispiele pro Label')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Anzahl der Beispiele')\n",
    "plt.xticks(ticks= np.arange(26), labels=np.arange(26))\n",
    "plt.legend(title='Datensatz')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each training and test case represents a label (0-25) as a one-to-one map for each alphabetic letter A-Z (and no cases for 9=J or 25=Z because of gesture motions)\n",
    "labelAsLetter = list(string.ascii_lowercase)\n",
    "labelAsLetter.remove(\"j\")\n",
    "labelAsLetter.remove(\"z\")\n",
    "\n",
    "numbers = list(set(sign_train[\"label\"]))\n",
    "letterMapping = dict(zip(numbers, labelAsLetter))\n",
    "\n",
    "print(letterMapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select an index\n",
    "random_index = np.random.randint(0, len(sign_train))\n",
    "\n",
    "# Extract the label and image data\n",
    "label = sign_train.iloc[random_index, 0]\n",
    "letter = letterMapping.get(label)\n",
    "image_data = sign_train.iloc[random_index, 1:].values\n",
    "image_data = image_data.reshape(28, 28)  # Reshape the flattened image data to 28x28\n",
    "\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(image_data, cmap='gray')\n",
    "plt.title(f'Label: {letter}')\n",
    "plt.axis('off')  # Hide axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignLanguageDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "\n",
    "        self.X_data = dataset.iloc[:, 1:785].values\n",
    "        \n",
    "        # Normalize the features using RobustScaler\n",
    "        scaler = RobustScaler()\n",
    "        self.X_data = scaler.fit_transform(self.X_data)\n",
    "\n",
    "        # Convert features to PyTorch FloatTensor\n",
    "        self.X_data = torch.FloatTensor(self.X_data)\n",
    "\n",
    "        #reshape to needed image shape \n",
    "        self.X_data = self.X_data.reshape( -1, 1, 28, 28)\n",
    "\n",
    "        # Select and transform label column\n",
    "        self.y_data = torch.LongTensor(dataset.iloc[:, 0].values)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "\n",
    "train_data = SignLanguageDataset(sign_train)\n",
    "test_data = SignLanguageDataset(sign_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels= 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(64, 64 , kernel_size=5, stride=1, padding=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        #self.dropout= nn.Dropout(0.2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 64 , kernel_size=5, stride=1, padding=2)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * 3 * 3, 120)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc2 = nn.Linear(120, 25)\n",
    "\n",
    "    def forward(self, out):\n",
    "            out = self.conv1(out)\n",
    "            out = self.relu1(out)\n",
    "            out = self.pool1(out)\n",
    "\n",
    "            out = self.conv2(out)\n",
    "            out = self.relu2(out)\n",
    "            out = self.pool2(out)\n",
    "\n",
    "            out = self.conv3(out)\n",
    "            out = self.relu3(out)\n",
    "            out = self.pool3(out)\n",
    "\n",
    "            out = out.view(out.size(0), -1)  # Flatten the tensor\n",
    "\n",
    "            out = self.fc1(out)\n",
    "            out = self.relu4(out)\n",
    "            out = self.drop1(out)\n",
    "\n",
    "            out = self.fc2(out)\n",
    "            return out\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=SimpleCNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_configuration = {\n",
    "    'method': 'random',  # random, grid or bayes\n",
    "    'name': 'sweep-sign-language',\n",
    "    'metric': {'goal': 'maximize', 'name': 'weighted_F1_score'},\n",
    "    'parameters': \n",
    "    {\n",
    "        'batch_size': {'values': [16, 32, 64]},\n",
    "        'n_epochs': {'values': [10, 20, 25]},\n",
    "        'learning_rate': {'values': [0.01, 0.001, 0.0001]},\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(train_loader, model, loss, optimizer):\n",
    "    loss_epoch = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "\n",
    "        y_pred = model(X_batch)\n",
    "        batch_train_loss = loss(y_pred, y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        batch_train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_epoch += batch_train_loss.item()\n",
    "\n",
    "    return loss_epoch/len(train_loader)\n",
    "\n",
    "def evaluate_one_epoch(test_loader, model, loss):\n",
    "    test_loss = 0.0\n",
    "    true_test_labels = []\n",
    "    pred_test_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_test_batch, y_test_batch in test_loader: \n",
    "            y_test_pred = model(X_test_batch)\n",
    "            batch_test_loss = loss(y_test_pred, y_test_batch)\n",
    "            test_loss += batch_test_loss.item()\n",
    "\n",
    "            predicted = torch.argmax(y_test_pred, 1)\n",
    "            true_test_labels.extend(y_test_batch.numpy())\n",
    "            pred_test_labels.extend(predicted.numpy())\n",
    "\n",
    "    avg_test_loss = test_loss/len(test_loader)\n",
    "    weighted_F1_score = f1_score(true_test_labels, pred_test_labels, average=\"weighted\")\n",
    "\n",
    "    return  avg_test_loss, weighted_F1_score\n",
    "\n",
    "\n",
    "def train():\n",
    "    wandb.init()\n",
    "    \n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=wandb.config.batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=wandb.config.batch_size, shuffle=False)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
    "\n",
    "    best_weighted_f1 = 0\n",
    "\n",
    "    for epoch in range(wandb.config.n_epochs): \n",
    "\n",
    "        train_loss = train_one_epoch(train_loader, model, loss, optimizer)\n",
    "\n",
    "        test_loss, weighted_F1_score = evaluate_one_epoch(test_loader, model, loss)\n",
    "\n",
    "        wandb.log({\n",
    "            'epoch': epoch, \n",
    "            'train_loss': train_loss,\n",
    "            'test_loss': test_loss,\n",
    "            'weighted_F1': weighted_F1_score\n",
    "      })\n",
    "         \n",
    "        if (epoch+1)%5==0:\n",
    "             print(f'Epoch {epoch+1}/{wandb.config.batch_size, wandb.config.learning_rate, wandb.config.n_epochs}, loss {train_loss:.5f}, val_loss {test_loss:.5f}, weighted_F1 {weighted_F1_score:.5f}')\n",
    "    \n",
    "    if weighted_F1_score > best_weighted_f1:\n",
    "            best_weighted_f1 = weighted_F1_score\n",
    "            torch.save(model.state_dict(), 'best_model.pth')            \n",
    "            wandb.save('best_model.pth')\n",
    "    return model\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project='machine-learning-project')\n",
    "wandb.agent(sweep_id, function=train, count=2)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load best model\n",
    "model.load_state_dict(torch.load(\"best_model2.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Classification ability of used Model again with random index, but with true and predicted label\n",
    "random_index = random.randint(0, len(test_data) - 1)\n",
    "random_image, true_label = test_data[random_index]\n",
    "random_image = random_image.unsqueeze(0)  # shape: (1, 1, 28, 28)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(random_image)\n",
    "    predicted_label = torch.argmax(output, 1)\n",
    "    predicted_class = letterMapping[predicted_label.item()]\n",
    "    true_class = letterMapping[true_label.item()]\n",
    "\n",
    "    image_to_show = random_image.reshape(28, 28)\n",
    "\n",
    "plt.imshow(image_to_show, cmap='gray')\n",
    "plt.title(f'Predicted: {predicted_class}, True Label: {true_class}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize Model prediction using confusion matrix\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "def evaluate_model_confusion_matrix(test_loader, model):\n",
    "    true_test_labels = []\n",
    "    pred_test_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_test_batch, y_test_batch in test_loader: \n",
    "            y_test_pred = model(X_test_batch)\n",
    "            predicted = torch.argmax(y_test_pred, 1)\n",
    "            true_test_labels.extend(y_test_batch.numpy())\n",
    "            pred_test_labels.extend(predicted.numpy())\n",
    "\n",
    "    weighted_F1_score = f1_score(true_test_labels, pred_test_labels, average=\"weighted\")\n",
    "\n",
    "    return weighted_F1_score, true_test_labels, pred_test_labels\n",
    "\n",
    "f1, y_true, y_pred = evaluate_model_confusion_matrix(test_loader=test_loader, model=model)\n",
    "\n",
    "print(\"weighted average f1 score: \" + str(f1))\n",
    "\n",
    "\n",
    "# Creating the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Mapping the confusion matrix labels to letters\n",
    "letter_mapping = {0: 'a', 1: 'b', 2: 'c', 3: 'd', 4: 'e', 5: 'f', 6: 'g', 7: 'h', 8: 'i', 10: 'k', 11: 'l', 12: 'm', 13: 'n', 14: 'o', 15: 'p', 16: 'q', 17: 'r', 18: 's', 19: 't', 20: 'u', 21: 'v', 22: 'w', 23: 'x', 24: 'y'}\n",
    "labels = [letter_mapping[i] for i in sorted(letter_mapping.keys())]\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
